{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "Tensorflow 2 provide tf.distribute.Strategy API for distributed training. All we have to do is to follow two procedures. \n",
    "1. Wraping components with a distributed strategy\n",
    "2. Defining training procedure\n",
    "\n",
    "Notice! Current tensorflow 2.0 is very unstable in handling variable-lenghts inputs. We have to modify our codes in complicated ways to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wraping models with a distributed strategy\n",
    "Tensorflow provide wide range of distributed strategies, such as mirrored strategy, TPU strategy. One of the widely used strategy is mirrored strategy. Mirrored strategy is basically the same to data parrelel in pytorch. It deploies replica of a model at each GPU device and distributes inputs evenly to them. Gradients are computed at each GPU device and they updated after merging. We can apply mirrored strategy by `calling strategy instance` and `wrapping components (model, optimizer, dataset)` with the strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#create distributed strategy instance\n",
    "strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class SimpleModel(keras.Model):\n",
    "    def __init__(self, n_class):\n",
    "        super(SimpleModel,self).__init__()\n",
    "        self.output_layer = keras.layers.Dense(n_class)\n",
    "    \n",
    "    def call(self,x):\n",
    "        return self.output_layer(x)\n",
    "\n",
    "#dataset\n",
    "\n",
    "X = tf.random.normal((10000,100))\n",
    "Y = tf.random.uniform((10000,),0,5,dtype=tf.int64)\n",
    "\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X,Y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wraping the model, optimizer\n",
    "with strategy.scope():\n",
    "    model = SimpleModel(5)\n",
    "    optimizer = keras.optimizers.Adam(0.001)\n",
    "    \n",
    "#create distributed dataset\n",
    "dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "# we have to use dataset with an iterator instance to prevent errors.  \n",
    "dist_dataset = iter(dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining training procedure\n",
    "\n",
    "The concept is very intuitive. We first define the training step for each device and distribute them into multiple GPUs. \n",
    "\n",
    "However, current tensorflow 2.0 is unstable in handling variable-length inputs, so we must circumvent it to avoid potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current tensorflow 2.0 only operate when training procedure is decorated with @tf.function\n",
    "@tf.function\n",
    "def train_step():\n",
    "    # tensorflow 2.0 can't recognize variable-lenghts inputs when they are passed as arguements.\n",
    "    # We need to generate create inputs within the method by calling next() \n",
    "    x,y = next(dist_dataset)\n",
    "    def step_fn(x,y):\n",
    "        # this inner method is training step for each device\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_ = model(x)\n",
    "            per_device_loss = keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_, from_logits=True)\n",
    "            \n",
    "            # we scaled the loss by the number of GPUs.\n",
    "            # This is important because all the replicas are training in sync\n",
    "            losses = tf.reduce_sum(per_device_loss) * (1.0 / strategy.num_replicas_in_sync)\n",
    "        step_grad = tape.gradient(losses, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(step_grad, model.trainable_variables))\n",
    "        \n",
    "        # return losses to keep track the loss values\n",
    "        return tf.reduce_mean(losses)[None]\n",
    "\n",
    "    example_loss = strategy.experimental_run_v2(step_fn, args=(x, y))\n",
    "    \n",
    "    # Normally, we record loss with the mean values over batch losses.\n",
    "    # But current tensorflow 2.0 raises an error when averaging the losses.\n",
    "    # To compute mean losses over global batch, we first get scaled mean loss per each device, and adding them up.\n",
    "    # This is very complicated. I hope this would be fixed in the next version.\n",
    "    losses_sum = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, example_loss,axis=0)\n",
    "    return losses_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Epoch\n",
    "These procedures can be applied for the entire datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataset,model,optimizer):\n",
    "    pbar = tf.keras.utils.Progbar(1000)\n",
    "    pbar_cnt = 0\n",
    "    # we must know the amount of iteration per epoch\n",
    "    # It's because we can't pass inputs to the train_step(), and generate inputs within the train_step() methods.\n",
    "    for i in range(1000):\n",
    "        pbar_cnt+=1\n",
    "        computed_loss = train_step()\n",
    "        pbar.update(pbar_cnt, [['loss',computed_loss]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 944us/step - loss: 15.4272\n"
     ]
    }
   ],
   "source": [
    "train_epoch(dataset,model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
